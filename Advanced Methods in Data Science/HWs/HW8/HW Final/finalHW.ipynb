{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "706ff59b",
   "metadata": {},
   "source": [
    "# INFO 371 - Final Homework \n",
    "\n",
    "For this homework assignment, we are going to ask to you to put together everything you have learned throughout the quarter to analyze 4 very different datasets! The goal is for you to gain experience and intuition for how each of the supervised learning algorithms perform in different scenarios. \n",
    "\n",
    "# Datasets \n",
    "There are four datasets you will need to analyze (one of which you have worked with in past asignments!) \n",
    "\n",
    "## Dataset 1: Soybean Dataset\n",
    "With this dataset (``soybean.csv``) your goal is to predict which of the four diseases a particular soy-bean plant has. This datset was part of a project used to build a survey to diagnose crops (you can read more about this work [here](http://www.mli.gmu.edu/papers/79-80/80-2.pdf). The dataset has the following features: \n",
    "\n",
    "* Month: which month was the plant disease found, represented as a number meaning april is 4 ect.\n",
    "* Plant Stand: describes how much of the soybean plant is above the soil \n",
    "    * 0 is avg,  1 is below avg\n",
    "* Precipitation:  0: below avg, 1: avg, 2: above avg\n",
    "* temp:  0: below avg, 1: avg, 2: above avg\n",
    "* hail: 0 is no, 1 is yes \n",
    "* crop-hist\t\n",
    "    * 0: diff-lst-year\n",
    "    * 1: same-lst-yr\n",
    "    * 2: same-lst-two-yrs\n",
    "    * 3: same-lst-sev-yrs\n",
    "* area-damaged\n",
    "    * 0: scattered\n",
    "    * 1: low-areas\n",
    "    * 2: upper-areas\n",
    "    * 3:whole-field\n",
    "* severity\n",
    "    * 0 minor,1: potentially severe, 2: severe\n",
    "* seed-tmt: 0: None, 1: used fungicide \n",
    "* germination: How much of the plants sprouted\n",
    "    * 0: 90-100%, 1: 80-89%, 2: 80% or less\t\n",
    "* plant-growth: 0: normal, 1: abnormal \n",
    "* leaves: 0: normal, 1: abnormal \n",
    "* leafspots-halo: 0: absent: 1: has yellow halos, 2: has halos, but not yellow halos\n",
    "* leafspots-margin: 0: water-soaked margin,1: no margin, 2: does not apply \t\n",
    "* leafspot-size: 0: less than 1/8 in, 1: bigger than 1/8 in, 2: does not apply\n",
    "* leaf-shread: \t0: absent, 1: present\n",
    "* leaf-malf: 0: absent, 1: present\n",
    "* leaf-mild: 0: absent, 1: present on upper surface ,2: present on lower surface\n",
    "* stem: 0: normal, 1: not normal \n",
    "* lodging: 0: absent, 1: present\n",
    "* stem-cankers: 0: absent, 1: found below soil, 2: found above soil, 3: found above second node\n",
    "* canker-lesion\t0: absent,1: brown color ,2: dark brown to black color, 3: tan\n",
    "* fruiting-bodies: \t0: absent, 1: present\n",
    "* extern_decay: 0: absent, 1: firm and dry, 2: watery \n",
    "* mycelium: 0: absent, 1: present\t\n",
    "* int-discolor: 0: none, 1: black, 2: brown \n",
    "* sclerotia: 0: absent, 1: present\t\n",
    "* fruit-pods: 0: normal, 1: diseased, 2: few present, 3: does not apply \t\n",
    "* fruit-spots: 0: absent, 1: colored, 2: brown with black specks, 3: distorted, 4: does not apply\t\n",
    "* seed: 0: normal, 1: not normal\n",
    "* mold-groth: 0: absent, 1: present\n",
    "* seed-discolor: 0: absent, 1: present\n",
    "* seed-size: 0: average, 1: below average \t\n",
    "* shriveling: 0: absent, 1: present\t\n",
    "* roots\t: 0: normal, 1: diseased \n",
    "\n",
    "\n",
    "* Label: which of the diseases that plat has\n",
    "    * D1: diaporthe stem canker\n",
    "    * D2: charcoal rot\n",
    "    * D3: rhizoctonia root rot\n",
    "    * D4: phytophthora rot\n",
    "\n",
    "\n",
    "## Dataset 2: Iris Dataset\n",
    "With this dataset (``iris.csv``), your goal to categorize Iris flowers based on the four measurements (aka: features) into the correct species (aka: labels) setosa, virginica, and versicolor. The data contains 50 flowers of each species (150 in total), and four measurements for each species (petal length and width, and sepal length and width). All of these are numeric measures.\n",
    "\n",
    "## Dataset 3: Skin Segmentation Dataset\n",
    "With this dataset (``Skin_NonSkin.tsv``), your goal is to predict whether a particular pixel is a skin tone or not a skin tone. Each row in the dataset is a pixel taken from a random image. The features are the values in the B, G, R color space (i.e blue, green, red). \n",
    "\n",
    "## Dataset 4: Taiwanese Bankruptcy Prediction Dataset \n",
    "With this dataset (``bankruptcy.csv``), your goal is to predict whether a company went backrupt or not. This dataset was collected from the Taiwan Economic Journal for the years 1999 to 2009. Company bankruptcy was defined based on the business regulations of the Taiwan Stock Exchange. The dataset has the following features: \n",
    "\n",
    "* Bankrupt?: Class label\n",
    "* ROA(C) before interest and depreciation before interest: Return On Total Assets(C)\n",
    "* ROA(A) before interest and % after tax: Return On Total Assets(A)\n",
    "* ROA(B) before interest and depreciation after tax: Return On Total Assets(B)\n",
    "* Operating Gross Margin: Gross Profit/Net Sales\n",
    "* Realized Sales Gross Margin: Realized Gross Profit/Net Sales\n",
    "* Operating Profit Rate: Operating Income/Net Sales\n",
    "* Pre-tax net Interest Rate: Pre-Tax Income/Net Sales\n",
    "* After-tax net Interest Rate: Net Income/Net Sales\n",
    "* Non-industry income and expenditure/revenue: Net Non-operating Income Ratio\n",
    "* Continuous interest rate (after tax): Net Income-Exclude Disposal Gain or Loss/Net Sales\n",
    "* Operating Expense Rate: Operating Expenses/Net Sales\n",
    "* Research and development expense rate: (Research and Development Expenses)/Net Sales\n",
    "* Cash flow rate: Cash Flow from Operating/Current Liabilities\n",
    "* Interest-bearing debt interest rate: Interest-bearing Debt/Equity\n",
    "* Tax rate (A): Effective Tax Rate\n",
    "* Net Value Per Share (B): Book Value Per Share(B)\n",
    "* Net Value Per Share (A): Book Value Per Share(A)\n",
    "* Net Value Per Share (C): Book Value Per Share(C)\n",
    "* Persistent EPS in the Last Four Seasons: EPS-Net Income\n",
    "* Cash Flow Per Share\n",
    "* Revenue Per Share (Yuan ¥): Sales Per Share\n",
    "* Operating Profit Per Share (Yuan ¥): Operating Income Per Share\n",
    "* Per Share Net profit before tax (Yuan ¥): Pretax Income Per Share\n",
    "* Realized Sales Gross Profit Growth Rate\n",
    "* Operating Profit Growth Rate: Operating Income Growth\n",
    "* After-tax Net Profit Growth Rate: Net Income Growth\n",
    "* Regular Net Profit Growth Rate: Continuing Operating Income after Tax Growth\n",
    "* Continuous Net Profit Growth Rate: Net Income-Excluding Disposal Gain or Loss Growth\n",
    "* Total Asset Growth Rate: Total Asset Growth\n",
    "* Net Value Growth Rate: Total Equity Growth\n",
    "* Total Asset Return Growth Rate Ratio: Return on Total Asset Growth\n",
    "* Cash Reinvestment %: Cash Reinvestment Ratio\n",
    "* Current Ratio\n",
    "* Quick Ratio: Acid Test\n",
    "* Interest Expense Ratio: Interest Expenses/Total Revenue\n",
    "* Total debt/Total net worth: Total Liability/Equity Ratio\n",
    "* Debt ratio %: Liability/Total Assets\n",
    "* Net worth/Assets: Equity/Total Assets\n",
    "* Long-term fund suitability ratio (A): (Long-term Liability+Equity)/Fixed Assets\n",
    "* Borrowing dependency: Cost of Interest-bearing Debt\n",
    "* Contingent liabilities/Net worth: Contingent Liability/Equity\n",
    "* Operating profit/Paid-in capital: Operating Income/Capital\n",
    "* Net profit before tax/Paid-in capital: Pretax Income/Capital\n",
    "* Inventory and accounts receivable/Net value: (Inventory+Accounts Receivables)/Equity\n",
    "* Total Asset Turnover\n",
    "* Accounts Receivable Turnover\n",
    "* Average Collection Days: Days Receivable Outstanding\n",
    "* Inventory Turnover Rate (times)\n",
    "* Fixed Assets Turnover Frequency\n",
    "* Net Worth Turnover Rate (times): Equity Turnover\n",
    "* Revenue per person: Sales Per Employee\n",
    "* Operating profit per person: Operation Income Per Employee\n",
    "* Allocation rate per person: Fixed Assets Per Employee\n",
    "* Working Capital to Total Assets\n",
    "* Quick Assets/Total Assets\n",
    "* Current Assets/Total Assets\n",
    "* Cash/Total Assets\n",
    "* Quick Assets/Current Liability\n",
    "* Cash/Current Liability\n",
    "* Current Liability to Assets\n",
    "* Operating Funds to Liability\n",
    "* Inventory/Working Capital\n",
    "* Inventory/Current Liability\n",
    "* Current Liabilities/Liability\n",
    "* Working Capital/Equity\n",
    "* Current Liabilities/Equity\n",
    "* Long-term Liability to Current Assets\n",
    "* Retained Earnings to Total Assets\n",
    "* Total income/Total expense\n",
    "* Total expense/Assets\n",
    "* Current Asset Turnover Rate: Current Assets to Sales\n",
    "* Quick Asset Turnover Rate: Quick Assets to Sales\n",
    "* Working capitcal Turnover Rate: Working Capital to Sales\n",
    "* Cash Turnover Rate: Cash to Sales\n",
    "* Cash Flow to Sales\n",
    "* Fixed Assets to Assets\n",
    "* Current Liability to Liability\n",
    "* Current Liability to Equity\n",
    "* Equity to Long-term Liability\n",
    "* Cash Flow to Total Assets\n",
    "* Cash Flow to Liability\n",
    "* CFO to Assets\n",
    "* Cash Flow to Equity\n",
    "* Current Liability to Current Assets\n",
    "* Liability-Assets Flag: 1 if Total Liability exceeds Total Assets, 0 otherwise\n",
    "* Net Income to Total Assets\n",
    "* Total assets to GNP price\n",
    "* No-credit Interval\n",
    "* Gross Profit to Sales\n",
    "* Net Income to Stockholder's Equity\n",
    "* Liability to Equity\n",
    "* Degree of Financial Leverage (DFL)\n",
    "* Interest Coverage Ratio (Interest expense to EBIT)\n",
    "* Net Income Flag: 1 if Net Income is Negative for the last two years, 0 otherwise\n",
    "* Equity to Liability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b651d67",
   "metadata": {},
   "source": [
    "---\n",
    "# Dataset Exploration \n",
    "\n",
    "1. Load in each dataset, then report on how many rows and columns each dataset has AND the label distibution (i.e how many data items are in each label -- for example, the iris dataset has 50 flowers across each of the 3 flower species labels)\n",
    "\n",
    "\n",
    "2. Based on this dataset size information and the given dataset decriptions alone, which of the supervised learning models (Logistic Regression, K-NN, Naive Bayes, SVMs, decision trees, random forests, or Neural Networks) do you think will peform best on each dataset? Which do you think will perform worst? Explain your reasoning for each choice. \n",
    "    NOTE: DO NOT RUN ANY EXPERIMENTS BEFOREHAND! You should use your intuition and class discussions for this! \n",
    "    \n",
    "\n",
    "3. Get a baseline accuracy using the naive model (i.e. a model where you assign the same label to all your testing data and that label is the one that appeared the most in your training data) for each of the datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5fbb8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of rows and columns in Dataset 1: (47, 36)\n",
      "The label distribution: \n",
      "D4    17\n",
      "D1    10\n",
      "D2    10\n",
      "D3    10\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "# of rows and columns in Dataset 2: (150, 5)\n",
      "The label distribution: \n",
      "setosa        50\n",
      "versicolor    50\n",
      "virginica     50\n",
      "Name: Species, dtype: int64\n",
      "\n",
      "# of rows and columns in Dataset 3: (245057, 4)\n",
      "The label distribution: \n",
      "2    194198\n",
      "1     50859\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "# of rows and columns in Dataset 4: (6819, 96)\n",
      "The label distribution: \n",
      "0    6599\n",
      "1     220\n",
      "Name: Bankrupt?, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#code goes here\n",
    "#1\n",
    "df1 = pd.read_csv(\"soybean.csv\")\n",
    "df2 = pd.read_csv(\"iris.csv\", sep = '\\t')\n",
    "df3 = pd.read_csv(\"Skin_NonSkin.tsv\", sep = '\\t')\n",
    "df4 = pd.read_csv(\"bankruptcy.csv\")\n",
    "\n",
    "print(\"# of rows and columns in Dataset 1: \" + str(df1.shape))\n",
    "print(\"The label distribution: \")\n",
    "print(str(df1['Label'].value_counts()))\n",
    "print()\n",
    "print(\"# of rows and columns in Dataset 2: \" + str(df2.shape))\n",
    "print(\"The label distribution: \")\n",
    "print(str(df2['Species'].value_counts()))\n",
    "print()\n",
    "print(\"# of rows and columns in Dataset 3: \" + str(df3.shape))\n",
    "print(\"The label distribution: \")\n",
    "print(str(df3['Label'].value_counts()))\n",
    "print()\n",
    "print(\"# of rows and columns in Dataset 4: \" + str(df4.shape))\n",
    "print(\"The label distribution: \")\n",
    "print(str(df4['Bankrupt?'].value_counts()))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15c656da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4b93ae",
   "metadata": {},
   "source": [
    "For `soybean.csv` dataset, I think that Logistic Regression, Naive Bayes, Decision Trees, and Random Forests supervised learning models will perform the best because the dataset size is small, and those models are suitable for small datasets and can handle non-linearities in the data. However, on the other hand, Neural Networks may not perform worst since the dataset contains more number of features, making the model too complicated to train effectively.\n",
    "\n",
    "For `iris.csv` dataset, I think that Logistic Regression, K-NN, Naive Bayes, SVMs, Decision Trees, Random Forests, and Neural Networks supervised learning models will perform the best because the dataset size is small and the number of columns is not too large, so most supervised learning models may perform the best on this datset. Therefore, since most supervised learning models may fit well to this dataset, I do not think that none of the models may perform worst on this dataset.\n",
    "\n",
    "For `Skin_NonSkin.tsv` dataset, I think that SVMs, Decision Trees, Random Forests, and Neural Networks supervised learning models will perform the best because the dataset size is large and there are multiple features, and they are the models that can handle high-dimensional data and non-linearities well in the data. However, on the other hand, Logistic Regression, K-NN, and Naive Bayes might not perform well on this dataset since they are not suitable for high-dimensional data.\n",
    "\n",
    "For `bankruptcy.csv` dataset, I think that Logistic Regression, SVMs, and Neural Networks supervised learning models will perform the best because the dataset has a moderate size and a large number of features. However, on the other hand, K-NN, Naive Bayes, Decision Trees, and Random Forests might not perform well on this dataset since they can hardly handle high-dimensional data and are more likely to overfit the data in such cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b23a1375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy in Dataset 1: 0.1\n",
      "Baseline Accuracy in Dataset 2: 0.2\n",
      "Baseline Accuracy in Dataset 3: 0.7906227046437607\n",
      "Baseline Accuracy in Dataset 4: 0.9626099706744868\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "names = ['Label', 'Species', 'Label', 'Bankrupt?']\n",
    "dfs = [df1, df2, df3, df4]\n",
    "\n",
    "def base_line(df, k):\n",
    "    features = df.drop(names[k], axis=1)\n",
    "    label = df[names[k]].to_frame()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2)\n",
    "    m = DummyClassifier(strategy = 'most_frequent')\n",
    "    m.fit(X_train, y_train)\n",
    "    print(\"Baseline Accuracy in Dataset \" + str(k+1) + \": \" + str(m.score(X_test, y_test)))\n",
    "\n",
    "k = 0\n",
    "for df in dfs:\n",
    "    base_line(df, k)\n",
    "    k = k + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d859e76",
   "metadata": {},
   "source": [
    "# Model Comparisons and Analysis\n",
    "\n",
    "4.Compare the average testing accuracy as well as runtime across each of the 4 datasets for each of the following experiment set-ups: \n",
    "\n",
    "    1. Logistic Regression \n",
    "\n",
    "    2. K-Nearest Neighbors (K-NN) for euclidean and manhattan distance and for k values = {5, 10, 15, 20, 25, 30, 35, 40, 45, 50}\n",
    "\n",
    "    3. Naive Bayes \n",
    "\n",
    "    4. Decision Trees using citerion = \"entropy\" across various values for max_depth  \n",
    "    \n",
    "    5. Random Forests\n",
    "    \n",
    "    6. Neural Networks\n",
    "\n",
    "Note: you may use k-fold cross validation for these experiments\n",
    "\n",
    "\n",
    "5. Were your predicitons from step 2 correct? Why or why not? \n",
    "\n",
    "\n",
    "6. Which model would you chose for each dataset? Explain your reasoning and use the results of your experiments and any domain knowledge you have to support your argument. \n",
    "\n",
    "\n",
    "7. Why do you think certain models performed better on some of your datasets versus others? Are there certain attributes/aspects of each dataset that some machine learning models prefer? \n",
    "\n",
    "\n",
    "8. Now repeat these experiments from question 4, but this time run PCA to reduce the dimensions of your feature set beforehand. Try experimenting with different values for the number of dimensions returned by PCA (you will likely need different values for each dataset since they are all different sizes). What happens to the performance as you reduce the dimesions? Do some models do better/worse? Does it depend on the dataset? Why is this happening? Explain your reasoning. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9388ec54",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy in Dataset 1: 0.3\n",
      "Logistic Regression Accuracy in Dataset 1: 1.0\n",
      "Runtime of Logistic Regression in Dataset 1: 0.02081465721130371 seconds\n",
      "\n",
      "Baseline Accuracy in Dataset 2: 0.2\n",
      "Logistic Regression Accuracy in Dataset 2: 1.0\n",
      "Runtime of Logistic Regression in Dataset 2: 0.01300358772277832 seconds\n",
      "\n",
      "Baseline Accuracy in Dataset 3: 0.7922141516363339\n",
      "Logistic Regression Accuracy in Dataset 3: 0.9187545907124786\n",
      "Runtime of Logistic Regression in Dataset 3: 0.5472304821014404 seconds\n",
      "\n",
      "Baseline Accuracy in Dataset 4: 0.969208211143695\n",
      "Logistic Regression Accuracy in Dataset 4: 0.9604105571847508\n",
      "Runtime of Logistic Regression in Dataset 4: 0.10043978691101074 seconds\n",
      "\n",
      "Baseline Accuracy in Dataset 1: 0.4\n",
      "Average Accuracy of K-Nearest Neighbors (K-NN) with distance euclidean in Dataset 1: 0.642857142857143\n",
      "Runtime of K-Nearest Neighbors (K-NN) with distance euclidean in Dataset 1: 0.0032854080200195312 seconds\n",
      "Average Accuracy of K-Nearest Neighbors (K-NN) with distance manhattan in Dataset 1: 0.7142857142857144\n",
      "Runtime of K-Nearest Neighbors (K-NN) with distance manhattan in Dataset 1: 0.0024340152740478516 seconds\n",
      "\n",
      "Baseline Accuracy in Dataset 2: 0.23333333333333334\n",
      "Average Accuracy of K-Nearest Neighbors (K-NN) with distance euclidean in Dataset 2: 0.9133333333333334\n",
      "Runtime of K-Nearest Neighbors (K-NN) with distance euclidean in Dataset 2: 0.003409075736999512 seconds\n",
      "Average Accuracy of K-Nearest Neighbors (K-NN) with distance manhattan in Dataset 2: 0.9166666666666667\n",
      "Runtime of K-Nearest Neighbors (K-NN) with distance manhattan in Dataset 2: 0.0037212371826171875 seconds\n",
      "\n",
      "Baseline Accuracy in Dataset 3: 0.7935607606300498\n",
      "Average Accuracy of K-Nearest Neighbors (K-NN) with distance euclidean in Dataset 3: 0.9991002203541989\n",
      "Runtime of K-Nearest Neighbors (K-NN) with distance euclidean in Dataset 3: 2.2845274448394775 seconds\n",
      "Average Accuracy of K-Nearest Neighbors (K-NN) with distance manhattan in Dataset 3: 0.9990777768709703\n",
      "Runtime of K-Nearest Neighbors (K-NN) with distance manhattan in Dataset 3: 2.874722146987915 seconds\n",
      "\n",
      "Baseline Accuracy in Dataset 4: 0.9640762463343109\n",
      "Average Accuracy of K-Nearest Neighbors (K-NN) with distance euclidean in Dataset 4: 0.9697947214076248\n",
      "Runtime of K-Nearest Neighbors (K-NN) with distance euclidean in Dataset 4: 0.25680420398712156 seconds\n",
      "Average Accuracy of K-Nearest Neighbors (K-NN) with distance manhattan in Dataset 4: 0.9697214076246337\n",
      "Runtime of K-Nearest Neighbors (K-NN) with distance manhattan in Dataset 4: 1.2123613119125367 seconds\n",
      "\n",
      "Baseline Accuracy in Dataset 1: 0.2\n",
      "Naive Bayes Accuracy in Dataset 1: 1.0\n",
      "Naive Bayes Runtime in Dataset 1: 0.0 seconds\n",
      "\n",
      "Baseline Accuracy in Dataset 2: 0.3\n",
      "Naive Bayes Accuracy in Dataset 2: 0.9333333333333333\n",
      "Naive Bayes Runtime in Dataset 2: 0.01060032844543457 seconds\n",
      "\n",
      "Baseline Accuracy in Dataset 3: 0.7944993062923366\n",
      "Naive Bayes Accuracy in Dataset 3: 0.9232840936913409\n",
      "Naive Bayes Runtime in Dataset 3: 0.04687333106994629 seconds\n",
      "\n",
      "Baseline Accuracy in Dataset 4: 0.968475073313783\n",
      "Naive Bayes Accuracy in Dataset 4: 0.07991202346041056\n",
      "Naive Bayes Runtime in Dataset 4: 0.0 seconds\n",
      "\n",
      "Baseline Accuracy in Dataset 1: 0.4\n",
      "Average accuracy of Decision Trees in Dataset 1: 0.96\n",
      "Runtime of Decision Trees in Dataset 1: 0.0015646696090698242 seconds\n",
      "\n",
      "Baseline Accuracy in Dataset 2: 0.26666666666666666\n",
      "Average accuracy of Decision Trees in Dataset 2: 0.8966666666666667\n",
      "Runtime of Decision Trees in Dataset 2: 0.004748821258544922 seconds\n",
      "\n",
      "Baseline Accuracy in Dataset 3: 0.7907859299763323\n",
      "Average accuracy of Decision Trees in Dataset 3: 0.9690524769444216\n",
      "Runtime of Decision Trees in Dataset 3: 0.194427752494812 seconds\n",
      "\n",
      "Baseline Accuracy in Dataset 4: 0.9648093841642229\n",
      "Average accuracy of Decision Trees in Dataset 4: 0.9616568914956012\n",
      "Runtime of Decision Trees in Dataset 4: 0.34650678634643556 seconds\n",
      "\n",
      "Baseline Accuracy in Dataset 1: 0.3\n",
      "Random Forest Accuracy in Dataset 1: 1.0\n",
      "Runtime for Random Forest in Dataset 1: 0.09707975387573242 seconds\n",
      "\n",
      "Baseline Accuracy in Dataset 2: 0.3\n",
      "Random Forest Accuracy in Dataset 2: 0.9666666666666667\n",
      "Runtime for Random Forest in Dataset 2: 0.11060309410095215 seconds\n",
      "\n",
      "Baseline Accuracy in Dataset 3: 0.7942748714600506\n",
      "Random Forest Accuracy in Dataset 3: 0.9995919366685709\n",
      "Runtime for Random Forest in Dataset 3: 12.00050139427185 seconds\n",
      "\n",
      "Baseline Accuracy in Dataset 4: 0.9655425219941349\n",
      "Random Forest Accuracy in Dataset 4: 0.966275659824047\n",
      "Runtime for Random Forest in Dataset 4: 3.882011651992798 seconds\n",
      "\n",
      "Baseline Accuracy in Dataset 1: 0.4\n",
      "Neural Network Accuracy in Dataset 1: 1.0\n",
      "Runtime for Neural Network in Dataset 1: 0.29423975944519043 seconds\n",
      "\n",
      "Baseline Accuracy in Dataset 2: 0.3\n",
      "Neural Network Accuracy in Dataset 2: 0.9666666666666667\n",
      "Runtime for Neural Network in Dataset 2: 0.5793824195861816 seconds\n",
      "\n",
      "Baseline Accuracy in Dataset 3: 0.7922345548029054\n",
      "Neural Network Accuracy in Dataset 3: 0.9986533910062841\n",
      "Runtime for Neural Network in Dataset 3: 68.60789942741394 seconds\n",
      "\n",
      "Baseline Accuracy in Dataset 4: 0.9604105571847508\n",
      "Neural Network Accuracy in Dataset 4: 0.9244868035190615\n",
      "Runtime for Neural Network in Dataset 4: 0.8555035591125488 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# code goes here \n",
    "#4\n",
    "# Logistic Regression\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def lr(df, k):\n",
    "    features = df.drop(names[k], axis=1)\n",
    "    label = df[names[k]].to_frame()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2)\n",
    "    s_time = time.time() \n",
    "    m = LogisticRegression()\n",
    "    m.fit(X_train, y_train)\n",
    "    score = m.score(X_test, y_test)\n",
    "    e_time = time.time()  \n",
    "    runtime = e_time - s_time\n",
    "    print(\"Logistic Regression Accuracy in Dataset \" + str(k+1) + \": \" + str(score))\n",
    "    print(\"Runtime of Logistic Regression in Dataset \" + str(k+1) + \": \" + str(runtime) + \" seconds\")\n",
    "k = 0\n",
    "for df in dfs:\n",
    "    base_line(df, k)\n",
    "    lr(df, k)\n",
    "    print()\n",
    "    k = k + 1\n",
    "    \n",
    "\n",
    "# K-Nearest Neighbors (K-NN) for euclidean and manhattan distance    \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def knn(df, k):\n",
    "    features = df.drop(names[k], axis=1)\n",
    "    label = df[names[k]].to_frame()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2)\n",
    "    for distance in ['euclidean', 'manhattan']:\n",
    "        accuracies = []\n",
    "        runtimes = []\n",
    "        if k == 0:\n",
    "            for i in range(5, 36, 5):\n",
    "                s_time = time.time()\n",
    "                m = KNeighborsClassifier(n_neighbors=i, metric=distance)\n",
    "                m.fit(X_train, y_train)\n",
    "                score = m.score(X_test, y_test)\n",
    "                e_time = time.time()\n",
    "                runtime = e_time - s_time\n",
    "                accuracies.append(score)\n",
    "                runtimes.append(runtime)\n",
    "        else:\n",
    "            for i in range(5, 51, 5):\n",
    "                s_time = time.time()\n",
    "                m = KNeighborsClassifier(n_neighbors=i, metric=distance)\n",
    "                m.fit(X_train, y_train)\n",
    "                score = m.score(X_test, y_test)\n",
    "                e_time = time.time()\n",
    "                runtime = e_time - s_time\n",
    "                accuracies.append(score)\n",
    "                runtimes.append(runtime)\n",
    "        avg_accuracy = np.mean(accuracies)\n",
    "        avg_runtime = np.mean(runtimes)\n",
    "        print(\"Average Accuracy of K-Nearest Neighbors (K-NN) with distance \" + distance + \" in Dataset \" + str(k+1) + \": \" + str(avg_accuracy))\n",
    "        print(\"Runtime of K-Nearest Neighbors (K-NN) with distance \" + distance + \" in Dataset \" + str(k+1) + \": \" + str(avg_runtime) + \" seconds\")\n",
    "k = 0\n",
    "for df in dfs:\n",
    "    base_line(df, k)\n",
    "    knn(df, k)\n",
    "    print()\n",
    "    k = k + 1\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "def nb(df, k):\n",
    "    features = df.drop(names[k], axis=1)\n",
    "    label = df[names[k]].to_frame()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2)\n",
    "    s_time = time.time() \n",
    "    m = GaussianNB()\n",
    "    m.fit(X_train, y_train)\n",
    "    score = m.score(X_test, y_test)\n",
    "    e_time = time.time() \n",
    "    runtime = e_time - s_time\n",
    "    print(\"Naive Bayes Accuracy in Dataset \" + str(k+1) + \": \" + str(score))\n",
    "    print(\"Naive Bayes Runtime in Dataset \" + str(k+1) + \": \" + str(runtime) + \" seconds\")\n",
    "k = 0\n",
    "for df in dfs:\n",
    "    base_line(df, k)\n",
    "    nb(df, k)\n",
    "    print()\n",
    "    k = k + 1\n",
    "\n",
    "    \n",
    "# Decision Trees using citerion = \"entropy\" across various values for max_depth\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def dt(df, k):\n",
    "    features = df.drop(names[k], axis=1)\n",
    "    label = df[names[k]].to_frame()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2)\n",
    "    accuracies = []\n",
    "    runtimes = []\n",
    "    for depth in range(1, 11):\n",
    "        s_time = time.time()\n",
    "        m = DecisionTreeClassifier(criterion=\"entropy\", max_depth=depth)\n",
    "        m.fit(X_train, y_train)\n",
    "        score = m.score(X_test, y_test)\n",
    "        e_time = time.time()\n",
    "        accuracies.append(score)\n",
    "        runtimes.append(e_time - s_time)\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    avg_runtime = np.mean(runtimes)\n",
    "    print(\"Average accuracy of Decision Trees in Dataset \" + str(k+1) + \": \" + str(avg_accuracy))\n",
    "    print(\"Runtime of Decision Trees in Dataset \" + str(k+1) + \": \" + str(avg_runtime) + \" seconds\")\n",
    "k = 0\n",
    "for df in dfs:\n",
    "    base_line(df, k)\n",
    "    dt(df, k)\n",
    "    print()\n",
    "    k = k + 1\n",
    "\n",
    "    \n",
    "# Random Forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def rf(df, k):\n",
    "    features = df.drop(names[k], axis=1)\n",
    "    label = df[names[k]].to_frame()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2)\n",
    "    s_time = time.time()\n",
    "    m = RandomForestClassifier()\n",
    "    m.fit(X_train, y_train)\n",
    "    score = m.score(X_test, y_test)\n",
    "    e_time = time.time()\n",
    "    runtime = e_time - s_time\n",
    "    print(\"Random Forest Accuracy in Dataset \" + str(k+1) + \": \" + str(score))\n",
    "    print(\"Runtime for Random Forest in Dataset \" + str(k+1) + \": \" + str(runtime) + \" seconds\")\n",
    "k = 0\n",
    "for df in dfs:\n",
    "    base_line(df, k)\n",
    "    rf(df, k)\n",
    "    print()\n",
    "    k = k + 1\n",
    "\n",
    "    \n",
    "# Neural Networks\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "def nn(df, k):\n",
    "    features = df.drop(names[k], axis=1)\n",
    "    label = df[names[k]].to_frame()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2)\n",
    "    s_time = time.time()\n",
    "    m = MLPClassifier(hidden_layer_sizes = (10,), max_iter=1000)\n",
    "    m.fit(X_train, y_train)\n",
    "    score = m.score(X_test, y_test)\n",
    "    e_time = time.time()\n",
    "    runtime = e_time - s_time\n",
    "    print(\"Neural Network Accuracy in Dataset \" + str(k+1) + \": \" + str(score))\n",
    "    print(\"Runtime for Neural Network in Dataset \" + str(k+1) + \": \" + str(runtime) + \" seconds\")\n",
    "k = 0\n",
    "for df in dfs:\n",
    "    base_line(df, k)\n",
    "    nn(df, k)\n",
    "    print()\n",
    "    k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a978372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd5b328",
   "metadata": {},
   "source": [
    "For the `soybean.csv` dataset, all of the predictions were quite correct, with the exception of the Neural Network, which resulted in an accuracy of 1.0 despite being expected to perform the worst. This could be attributed to the small size of the dataset and the less complexity of the features, which made it possible for the neural network to effectively train the data.\n",
    "\n",
    "For the `iris.csv` dataset, all of the predictions were correct as all of the models performed significantly well based on the assumptions I suggested in Step 2.\n",
    "\n",
    "For the `Skin_NonSkin.tsv` dataset, all of the predictions were quite correct, with the exception of the K-NN, which resulted in an accuracy of 0.99 despite being expected to perform the worst. I assume that the dataset is of a size suitable for K-NN to produce more precise predictions.\n",
    "\n",
    "For the `bankruptcy.csv` dataset, all of the predictions were correct, especially for Naive Bayes model, which resulted in an accuracy of 0.08. This is because the dataset has numerous features and a substantial number of observations that Naive Bayes struggles to handle effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f06e66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df08b032",
   "metadata": {},
   "source": [
    "Based on the observation,\n",
    "\n",
    "For the `soybean.csv` dataset, we should use Random Forests. This model is well-suited for this dataset with categorical features and can handle non-linear relationships between the features and the target variable. Furthermore, it has produced the highest accuracy when compared to the other models. Additionally, Random Forests is suitable for small datasets containing a small number of features.\n",
    "\n",
    "For the `iris.csv` dataset, given that it is small and has the least complex features compared to the other four datasets, we should use Logistic Regression. This model has also demonstrated the best runtime performance and significantly higher accuracy compared to other models.\n",
    "\n",
    "For the `Skin_NonSkin.tsv` dataset, since it is a binary classification problem with three features, we should use a Neural Network. This model is well-suited for image classification tasks and can automatically learn and extract useful features from the input images. Furthermore, it has outperformed other models in terms of accuracy.\n",
    "\n",
    "For the `bankruptcy.csv` dataset, given that the number of features is relatively large, we should use Logistic Regression. This model can effectively reduce the number of features by shrinking some of them to zero, and can handle high-dimensional data efficiently. Additionally, this model has produced significantly higher accuracy when compared to the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c220dac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280fd481",
   "metadata": {},
   "source": [
    "I think that the reason for the varying performance of each model on different datasets is because each model has unique requirements regarding the types and complexity of features, as well as the size of the dataset.\n",
    "\n",
    "Also, there are certain attributes and aspects of each dataset that some machine learning models prefer. Different machine learning models are designed to solve specific types of problems and may perform better than others in certain situations. Some of the factors that can affect the performance of different models on a particular dataset are:\n",
    "\n",
    "- Dataset size: Large datasets usually require models that can handle high-dimensional data efficiently, while small datasets may require simpler models.\n",
    "\n",
    "- Feature types: Some models work better with numerical data, while others are designed to handle categorical data or text data.\n",
    "\n",
    "- Feature complexity: Models that can handle non-linear relationships between the features and the target variable may perform better on datasets with complex feature interactions.\n",
    "\n",
    "- Label distribution: If the number of examples for one class is much larger than the other, some models may perform better than others.\n",
    "\n",
    "For example, in the `soybean.csv` dataset, Random Forests performed better than other models because the dataset has categorical features, and Random Forests are well-suited to handle categorical data and non-linear relationships between features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e6044ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy in Dataset 1: 1.0\n",
      "Runtime of Logistic Regression in Dataset 1: 0.031082868576049805 seconds\n",
      "Logistic Regression Accuracy (PCA) in Dataset 1: 1.0\n",
      "Runtime of Logistic Regression (PCA) in Dataset 1: 0.009765674086178051 seconds\n",
      "\n",
      "Logistic Regression Accuracy in Dataset 2: 1.0\n",
      "Runtime of Logistic Regression in Dataset 2: 0.0319972038269043 seconds\n",
      "Logistic Regression Accuracy (PCA) in Dataset 2: 0.9\n",
      "Runtime of Logistic Regression (PCA) in Dataset 2: 0.009354432423909506 seconds\n",
      "\n",
      "Logistic Regression Accuracy in Dataset 3: 0.9184077368807639\n",
      "Runtime of Logistic Regression in Dataset 3: 0.938220739364624 seconds\n",
      "Logistic Regression Accuracy (PCA) in Dataset 3: 0.9140720639843304\n",
      "Runtime of Logistic Regression (PCA) in Dataset 3: 0.26959288120269775 seconds\n",
      "\n",
      "Logistic Regression Accuracy in Dataset 4: 0.9699413489736071\n",
      "Runtime of Logistic Regression in Dataset 4: 0.23517227172851562 seconds\n",
      "Logistic Regression Accuracy (PCA) in Dataset 4: 0.9705930270446399\n",
      "Runtime of Logistic Regression (PCA) in Dataset 4: 0.018159680896335177 seconds\n",
      "\n",
      "Average Accuracy of K-Nearest Neighbors (K-NN) with distance euclidean in Dataset 1: 0.7714285714285714\n",
      "Runtime of K-Nearest Neighbors (K-NN) with distance euclidean in Dataset 1: 0.0044651031494140625 seconds\n",
      "Average Accuracy of K-Nearest Neighbors (K-NN) with distance manhattan in Dataset 1: 0.7714285714285714\n",
      "Runtime of K-Nearest Neighbors (K-NN) with distance manhattan in Dataset 1: 0.006695917674473354 seconds\n",
      "Average Accuracy of K-Nearest Neighbors (PCA) (K-NN) in Dataset 1: 0.7388655462184874\n",
      "Runtime of K-Nearest Neighbors (PCA) (K-NN) in Dataset 1: 0.0026647653900274703 seconds\n",
      "\n",
      "Average Accuracy of K-Nearest Neighbors (K-NN) with distance euclidean in Dataset 2: 0.9700000000000001\n",
      "Runtime of K-Nearest Neighbors (K-NN) with distance euclidean in Dataset 2: 0.004686379432678222 seconds\n",
      "Average Accuracy of K-Nearest Neighbors (K-NN) with distance manhattan in Dataset 2: 0.9766666666666668\n",
      "Runtime of K-Nearest Neighbors (K-NN) with distance manhattan in Dataset 2: 0.003125596046447754 seconds\n",
      "Average Accuracy of K-Nearest Neighbors (PCA) (K-NN) in Dataset 2: 0.8627777777777778\n",
      "Runtime of K-Nearest Neighbors (PCA) (K-NN) in Dataset 2: 0.0033370415369669594 seconds\n",
      "\n",
      "Average Accuracy of K-Nearest Neighbors (K-NN) with distance euclidean in Dataset 3: 0.9992226393536277\n",
      "Runtime of K-Nearest Neighbors (K-NN) with distance euclidean in Dataset 3: 2.9703537940979006 seconds\n",
      "Average Accuracy of K-Nearest Neighbors (K-NN) with distance manhattan in Dataset 3: 0.9991002203541989\n",
      "Runtime of K-Nearest Neighbors (K-NN) with distance manhattan in Dataset 3: 3.19575617313385 seconds\n",
      "Average Accuracy of K-Nearest Neighbors (PCA) (K-NN) in Dataset 3: 0.9939785154656002\n",
      "Runtime of K-Nearest Neighbors (PCA) (K-NN) in Dataset 3: 2.789391481876373 seconds\n",
      "\n",
      "Average Accuracy of K-Nearest Neighbors (K-NN) with distance euclidean in Dataset 4: 0.9734604105571847\n",
      "Runtime of K-Nearest Neighbors (K-NN) with distance euclidean in Dataset 4: 0.17943377494812013 seconds\n",
      "Average Accuracy of K-Nearest Neighbors (K-NN) with distance manhattan in Dataset 4: 0.9734604105571847\n",
      "Runtime of K-Nearest Neighbors (K-NN) with distance manhattan in Dataset 4: 1.18652503490448 seconds\n",
      "Average Accuracy of K-Nearest Neighbors (PCA) (K-NN) in Dataset 4: 0.9726865428478332\n",
      "Runtime of K-Nearest Neighbors (PCA) (K-NN) in Dataset 4: 0.12429612212710911 seconds\n",
      "\n",
      "Naive Bayes Accuracy in Dataset 1: 1.0\n",
      "Naive Bayes Runtime in Dataset 1: 0.0 seconds\n",
      "Naive Bayes Accuracy (PCA) in Dataset 1: 1.0\n",
      "Naive Bayes Runtime (PCA) in Dataset 1: 0.002774491029627183 seconds\n",
      "\n",
      "Naive Bayes Accuracy in Dataset 2: 0.9666666666666667\n",
      "Naive Bayes Runtime in Dataset 2: 0.015623331069946289 seconds\n",
      "Naive Bayes Accuracy (PCA) in Dataset 2: 0.9777777777777779\n",
      "Naive Bayes Runtime (PCA) in Dataset 2: 0.0 seconds\n",
      "\n",
      "Naive Bayes Accuracy in Dataset 3: 0.9242226393536277\n",
      "Naive Bayes Runtime in Dataset 3: 0.06303286552429199 seconds\n",
      "Naive Bayes Accuracy (PCA) in Dataset 3: 0.9657736880763894\n",
      "Naive Bayes Runtime (PCA) in Dataset 3: 0.06296038627624512 seconds\n",
      "\n",
      "Naive Bayes Accuracy in Dataset 4: 0.07258064516129033\n",
      "Naive Bayes Runtime in Dataset 4: 0.015622615814208984 seconds\n",
      "Naive Bayes Accuracy (PCA) in Dataset 4: 0.9578038449006192\n",
      "Naive Bayes Runtime (PCA) in Dataset 4: 0.0034708711836073133 seconds\n",
      "\n",
      "Average accuracy of Decision Trees in Dataset 1: 0.9400000000000001\n",
      "Runtime of Decision Trees in Dataset 1: 0.003124833106994629 seconds\n",
      "Average accuracy of Decision Trees (PCA) in Dataset 1: 0.8321123823248396\n",
      "Runtime of Decision Trees (PCA) in Dataset 1: 0.0019298693050102658 seconds\n",
      "\n",
      "Average accuracy of Decision Trees in Dataset 2: 0.9266666666666667\n",
      "Runtime of Decision Trees in Dataset 2: 0.0031252384185791017 seconds\n",
      "Average accuracy of Decision Trees (PCA) in Dataset 2: 0.8953703703703705\n",
      "Runtime of Decision Trees (PCA) in Dataset 2: 0.0015626152356465657 seconds\n",
      "\n",
      "Average accuracy of Decision Trees in Dataset 3: 0.9686097282298212\n",
      "Runtime of Decision Trees in Dataset 3: 0.20317084789276124 seconds\n",
      "Average accuracy of Decision Trees (PCA) in Dataset 3: 0.9829832490002449\n",
      "Runtime of Decision Trees (PCA) in Dataset 3: 0.19605548977851867 seconds\n",
      "\n",
      "Average accuracy of Decision Trees in Dataset 4: 0.9628299120234605\n",
      "Runtime of Decision Trees in Dataset 4: 0.37616443634033203 seconds\n",
      "Average accuracy of Decision Trees (PCA) in Dataset 4: 0.9588795080864974\n",
      "Runtime of Decision Trees (PCA) in Dataset 4: 0.023574488523565695 seconds\n",
      "\n",
      "Random Forest Accuracy in Dataset 1: 1.0\n",
      "Runtime for Random Forest in Dataset 1: 0.15399527549743652 seconds\n",
      "Random Forest Accuracy (PCA) in Dataset 1: 0.9941176470588234\n",
      "Runtime for Random Forest (PCA) in Dataset 1: 0.15114970768199248 seconds\n",
      "\n",
      "Random Forest Accuracy in Dataset 2: 0.9333333333333333\n",
      "Runtime for Random Forest in Dataset 2: 0.15613555908203125 seconds\n",
      "Random Forest Accuracy (PCA) in Dataset 2: 0.9111111111111111\n",
      "Runtime for Random Forest (PCA) in Dataset 2: 0.15668304761250815 seconds\n",
      "\n",
      "Random Forest Accuracy in Dataset 3: 0.9996735493348567\n",
      "Runtime for Random Forest in Dataset 3: 12.276384830474854 seconds\n",
      "Random Forest Accuracy (PCA) in Dataset 3: 0.9974292010119971\n",
      "Runtime for Random Forest (PCA) in Dataset 3: 18.175325393676758 seconds\n",
      "\n",
      "Random Forest Accuracy in Dataset 4: 0.9604105571847508\n",
      "Runtime for Random Forest in Dataset 4: 4.0543859004974365 seconds\n",
      "Random Forest Accuracy (PCA) in Dataset 4: 0.9685565330726621\n",
      "Runtime for Random Forest (PCA) in Dataset 4: 1.1867552598317463 seconds\n",
      "\n",
      "Neural Network Accuracy in Dataset 1: 1.0\n",
      "Runtime for Neural Network in Dataset 1: 0.38588571548461914 seconds\n",
      "Neural Network Accuracy (PCA) in Dataset 1: 0.9558823529411763\n",
      "Runtime for Neural Network (PCA) in Dataset 1: 0.3522088738048778 seconds\n",
      "\n",
      "Neural Network Accuracy in Dataset 2: 0.9666666666666667\n",
      "Runtime for Neural Network in Dataset 2: 0.5514538288116455 seconds\n",
      "Neural Network Accuracy (PCA) in Dataset 2: 0.9333333333333332\n",
      "Runtime for Neural Network (PCA) in Dataset 2: 0.573580265045166 seconds\n",
      "\n",
      "Neural Network Accuracy in Dataset 3: 0.9984493593405697\n",
      "Runtime for Neural Network in Dataset 3: 32.18820810317993 seconds\n",
      "Neural Network Accuracy (PCA) in Dataset 3: 0.9904309148779891\n",
      "Runtime for Neural Network (PCA) in Dataset 3: 30.07091212272644 seconds\n",
      "\n",
      "Neural Network Accuracy in Dataset 4: 0.9728739002932552\n",
      "Runtime for Neural Network in Dataset 4: 0.5945611000061035 seconds\n",
      "Neural Network Accuracy (PCA) in Dataset 4: 0.9665200391006843\n",
      "Runtime for Neural Network (PCA) in Dataset 4: 2.3606233066982694 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Logistic Regression\n",
    "def lr_pca(df, k):\n",
    "    features = df.drop(names[k], axis=1)\n",
    "    label = df[names[k]].to_frame()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2)\n",
    "    if k == 3:\n",
    "        num_components = list(range(2, 11))\n",
    "    else:\n",
    "        num_components = list(range(2, df.shape[1]))\n",
    "    acc_list = []\n",
    "    runtime_list = []\n",
    "    for n_cmp in num_components:\n",
    "        sc = StandardScaler()\n",
    "        X_train_std = sc.fit_transform(X_train)\n",
    "        X_test_std = sc.transform(X_test)\n",
    "        pca = PCA(n_components = n_cmp)\n",
    "        X_train_pca = pca.fit_transform(X_train_std)\n",
    "        X_test_pca = pca.transform(X_test_std)\n",
    "        s_time = time.time() \n",
    "        m = LogisticRegression()\n",
    "        m.fit(X_train_pca, y_train)\n",
    "        score = m.score(X_test_pca, y_test)\n",
    "        e_time = time.time() \n",
    "        runtime = e_time - s_time\n",
    "        acc_list.append(score)\n",
    "        runtime_list.append(runtime)\n",
    "    print(\"Logistic Regression Accuracy (PCA) in Dataset \" + str(k+1) + \": \" + str(np.mean(acc_list)))\n",
    "    print(\"Runtime of Logistic Regression (PCA) in Dataset \" + str(k+1) + \": \" + str(np.mean(runtime_list)) + \" seconds\")\n",
    "k = 0\n",
    "for df in dfs:\n",
    "    lr(df, k)\n",
    "    lr_pca(df, k)\n",
    "    print()\n",
    "    k = k + 1\n",
    "    \n",
    "\n",
    "# K-Nearest Neighbors (K-NN) for euclidean and manhattan distance    \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def knn_pca(df, k):\n",
    "    features = df.drop(names[k], axis=1)\n",
    "    label = df[names[k]].to_frame()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2)\n",
    "    if k == 3:\n",
    "        num_components = list(range(2, 11))\n",
    "    else:\n",
    "        num_components = list(range(2, df.shape[1]))\n",
    "    acc_list = []\n",
    "    runtime_list = []\n",
    "    for n_cmp in num_components:\n",
    "        sc = StandardScaler()\n",
    "        X_train_std = sc.fit_transform(X_train)\n",
    "        X_test_std = sc.transform(X_test)\n",
    "        \n",
    "        pca = PCA(n_components = n_cmp)\n",
    "        X_train_pca = pca.fit_transform(X_train_std)\n",
    "        X_test_pca = pca.transform(X_test_std)\n",
    "\n",
    "        for distance in ['euclidean', 'manhattan']:\n",
    "            accuracies = []\n",
    "            runtimes = []\n",
    "            if k == 0:\n",
    "                for i in range(5, 36, 5):\n",
    "                    s_time = time.time()\n",
    "                    m = KNeighborsClassifier(n_neighbors=i, metric=distance)\n",
    "                    m.fit(X_train_pca, y_train)\n",
    "                    score = m.score(X_test_pca, y_test)\n",
    "                    e_time = time.time()\n",
    "                    runtime = e_time - s_time\n",
    "                    accuracies.append(score)\n",
    "                    runtimes.append(runtime)\n",
    "            else:\n",
    "                for i in range(5, 51, 5):\n",
    "                    s_time = time.time()\n",
    "                    m = KNeighborsClassifier(n_neighbors=i, metric=distance)\n",
    "                    m.fit(X_train_pca, y_train)\n",
    "                    score = m.score(X_test_pca, y_test)\n",
    "                    e_time = time.time()\n",
    "                    runtime = e_time - s_time\n",
    "                    accuracies.append(score)\n",
    "                    runtimes.append(runtime)\n",
    "            acc_list.append(np.mean(accuracies))\n",
    "            runtime_list.append(np.mean(runtimes))\n",
    "    print(\"Average Accuracy of K-Nearest Neighbors (PCA) (K-NN) in Dataset \" + str(k+1) + \": \" + str(np.mean(acc_list)))\n",
    "    print(\"Runtime of K-Nearest Neighbors (PCA) (K-NN) in Dataset \" + str(k+1) + \": \" + str(np.mean(runtime_list)) + \" seconds\")\n",
    "k = 0\n",
    "for df in dfs:\n",
    "    knn(df, k)\n",
    "    knn_pca(df, k)\n",
    "    print()\n",
    "    k = k + 1\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "def nb_pca(df, k):\n",
    "    features = df.drop(names[k], axis=1)\n",
    "    label = df[names[k]].to_frame()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2)\n",
    "    if k == 3:\n",
    "        num_components = list(range(2, 11))\n",
    "    else:\n",
    "        num_components = list(range(2, df.shape[1]))\n",
    "    acc_list = []\n",
    "    runtime_list = []\n",
    "    for n_cmp in num_components:\n",
    "        sc = StandardScaler()\n",
    "        X_train_std = sc.fit_transform(X_train)\n",
    "        X_test_std = sc.transform(X_test)\n",
    "        pca = PCA(n_components = n_cmp)\n",
    "        X_train_pca = pca.fit_transform(X_train_std)\n",
    "        X_test_pca = pca.transform(X_test_std)\n",
    "        s_time = time.time() \n",
    "        m = GaussianNB()\n",
    "        m.fit(X_train_pca, y_train)\n",
    "        score = m.score(X_test_pca, y_test)\n",
    "        e_time = time.time()  \n",
    "        runtime = e_time - s_time\n",
    "        acc_list.append(score)\n",
    "        runtime_list.append(runtime)\n",
    "    print(\"Naive Bayes Accuracy (PCA) in Dataset \" + str(k+1) + \": \" + str(np.mean(acc_list)))\n",
    "    print(\"Naive Bayes Runtime (PCA) in Dataset \" + str(k+1) + \": \" + str(np.mean(runtime_list)) + \" seconds\")\n",
    "k = 0\n",
    "for df in dfs:\n",
    "    nb(df, k)\n",
    "    nb_pca(df, k)\n",
    "    print()\n",
    "    k = k + 1\n",
    "\n",
    "    \n",
    "# Decision Trees using citerion = \"entropy\" across various values for max_depth\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def dt_pca(df, k):\n",
    "    features = df.drop(names[k], axis=1)\n",
    "    label = df[names[k]].to_frame()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2)\n",
    "    if k == 3:\n",
    "        num_components = list(range(2, 11))\n",
    "    else:\n",
    "        num_components = list(range(2, df.shape[1]))\n",
    "    acc_list = []\n",
    "    runtime_list = []\n",
    "    accuracies = []\n",
    "    runtimes = []\n",
    "    for n_cmp in num_components:\n",
    "        sc = StandardScaler()\n",
    "        X_train_std = sc.fit_transform(X_train)\n",
    "        X_test_std = sc.transform(X_test)\n",
    "        pca = PCA(n_components = n_cmp)\n",
    "        X_train_pca = pca.fit_transform(X_train_std)\n",
    "        X_test_pca = pca.transform(X_test_std)\n",
    "        for depth in range(1, 11):\n",
    "            s_time = time.time()\n",
    "            m = DecisionTreeClassifier(criterion=\"entropy\", max_depth=depth)\n",
    "            m.fit(X_train_pca, y_train)\n",
    "            score = m.score(X_test_pca, y_test)\n",
    "            e_time = time.time()\n",
    "            accuracies.append(score)\n",
    "            runtimes.append(e_time - s_time)\n",
    "        acc_list.append(np.mean(accuracies))\n",
    "        runtime_list.append(np.mean(runtimes))\n",
    "    print(\"Average accuracy of Decision Trees (PCA) in Dataset \" + str(k+1) + \": \" + str(np.mean(acc_list)))\n",
    "    print(\"Runtime of Decision Trees (PCA) in Dataset \" + str(k+1) + \": \" + str(np.mean(runtime_list)) + \" seconds\")\n",
    "k = 0\n",
    "for df in dfs:\n",
    "    dt(df, k)\n",
    "    dt_pca(df, k)\n",
    "    print()\n",
    "    k = k + 1\n",
    "\n",
    "    \n",
    "# Random Forests    \n",
    "def rf_pca(df, k):\n",
    "    features = df.drop(names[k], axis=1)\n",
    "    label = df[names[k]].to_frame()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2)\n",
    "    if k == 3:\n",
    "        num_components = list(range(2, 11))\n",
    "    else:\n",
    "        num_components = list(range(2, df.shape[1]))\n",
    "    acc_list = []\n",
    "    runtime_list = []\n",
    "    for n_cmp in num_components:\n",
    "        sc = StandardScaler()\n",
    "        X_train_std = sc.fit_transform(X_train)\n",
    "        X_test_std = sc.transform(X_test)\n",
    "        pca = PCA(n_components = n_cmp)\n",
    "        X_train_pca = pca.fit_transform(X_train_std)\n",
    "        X_test_pca = pca.transform(X_test_std)\n",
    "        s_time = time.time() \n",
    "        m = RandomForestClassifier()\n",
    "        m.fit(X_train_pca, y_train)\n",
    "        score = m.score(X_test_pca, y_test)\n",
    "        e_time = time.time()  \n",
    "        runtime = e_time - s_time\n",
    "        acc_list.append(score)\n",
    "        runtime_list.append(runtime)\n",
    "    print(\"Random Forest Accuracy (PCA) in Dataset \" + str(k+1) + \": \" + str(np.mean(acc_list)))\n",
    "    print(\"Runtime for Random Forest (PCA) in Dataset \" + str(k+1) + \": \" + str(np.mean(runtime_list)) + \" seconds\")\n",
    "k = 0\n",
    "for df in dfs:\n",
    "    rf(df, k)\n",
    "    rf_pca(df, k)\n",
    "    print()\n",
    "    k = k + 1\n",
    "\n",
    "    \n",
    "# Neural Networks    \n",
    "def nn_pca(df, k):\n",
    "    features = df.drop(names[k], axis=1)\n",
    "    label = df[names[k]].to_frame()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2)\n",
    "    if k == 3:\n",
    "        num_components = list(range(2, 11))\n",
    "    else:\n",
    "        num_components = list(range(2, df.shape[1]))\n",
    "    acc_list = []\n",
    "    runtime_list = []\n",
    "    for n_cmp in num_components:\n",
    "        sc = StandardScaler()\n",
    "        X_train_std = sc.fit_transform(X_train)\n",
    "        X_test_std = sc.transform(X_test)\n",
    "        pca = PCA(n_components = n_cmp)\n",
    "        X_train_pca = pca.fit_transform(X_train_std)\n",
    "        X_test_pca = pca.transform(X_test_std)\n",
    "        s_time = time.time() \n",
    "        m = MLPClassifier(hidden_layer_sizes = (10,), max_iter=1000)\n",
    "        m.fit(X_train_pca, y_train)\n",
    "        score = m.score(X_test_pca, y_test)\n",
    "        e_time = time.time()  \n",
    "        runtime = e_time - s_time\n",
    "        acc_list.append(score)\n",
    "        runtime_list.append(runtime)\n",
    "    print(\"Neural Network Accuracy (PCA) in Dataset \" + str(k+1) + \": \" + str(np.mean(acc_list)))\n",
    "    print(\"Runtime for Neural Network (PCA) in Dataset \" + str(k+1) + \": \" + str(np.mean(runtime_list)) + \" seconds\")\n",
    "k = 0\n",
    "for df in dfs:\n",
    "    nn(df, k)\n",
    "    nn_pca(df, k)\n",
    "    print()\n",
    "    k = k + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d44e852",
   "metadata": {},
   "source": [
    "For the `soybean.csv` dataset, reducing the dimensionality of the feature set using PCA did not significantly improve the performance of the Random Forest model.\n",
    "\n",
    "For the `iris.csv` dataset, reducing the dimensionality of the feature set using PCA had a negative effect on the performance of the Logistic Regression model, with the best performance achieved using the original feature set.\n",
    "\n",
    "For the `Skin_NonSkin.tsv` dataset, reducing the dimensionality of the feature set using PCA significantly did not improve the performance of the Neural Network model.\n",
    "\n",
    "For the `bankruptcy.csv` dataset, reducing the dimensionality of the feature set using PCA had a positive impact on the performance of the Logistic Regression model, with the best performance achieved using the original feature set.\n",
    "\n",
    "Therefore, based on the observation, reducing the dimensionality of the feature set using PCA did not always result in better performance for all models and datasets. The effect of dimensionality reduction on performance depends on the dataset and the model used. Some models, such as Neural Networks, may benefit more from dimensionality reduction than others. \n",
    "\n",
    "Nonetheless, overall, all models using PCA demonstrated better runtime than those without using PCA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
